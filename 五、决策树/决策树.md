决策树是一种基本的分类、回归方法，呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程，可以认为是 if-then 规则的集合，也可以认为是定义在特征空间和类空间上的条件概率分布。主要优点是模型具有可读性，分类速度快，学习时，利用训练数据，根据损失函数最小化原则建立决策树模型。预测时，对于新的数据，利用决策树模型进行预测。

# 决策树模型与学习

## 决策树模型

定义：分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点（node）和有向边（directed edge）组成。结点分为内部结点（internal node）和叶结点（leaf node），内部结点表示一个特征或属性，叶结点表示一个类

用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点，这时，每个子结点都对应于该特征的一个取值，如此，递归的对实例进行测试并分配，直到达到叶结点。最后将实例分到叶结点的类中。

下图是一个决策树的示意图，圆形表示内部节点，方形表示叶结点

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210319162204141.png" alt="image-20210319162204141" style="zoom:50%;" />

## if-then规则

决策树转换为 if-then 规则过程：由根结点到叶结点的每一条路径构成一条规则，路径上内部结点的特征对应于规则的条件，叶结点的类对应于规程的结论。决策树路径或 if-then 规则集合互斥且完备。这就是说每个实例都只被一条路径或规则所覆盖

## 条件概率分布

决策树还表示给定特征条件下类的条件概率分布。这一条件概率分布定义在特征空间的一个划分（partition）上，将特征空间划分为互不相干的单元（cell）或区域（region）。并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的一条路径对应于划分中的一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。假设 X 为特征的随机变量，Y 为类的随机变量，那么这个条件概率分布可表示为 P(Y|X)，X 取值于给定划分下单元的集合，Y 取值于类的集合。各叶结点上的条件概率往往偏向于一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去

图（a）表示了一个特征空间的划分，图中的大正方形表示特征空间，每个小矩形表示一个单元，特征空间划分上的单元构成一个集合，X 取值于单元的集合。假设只有两类，即 Y 的取值为 +1 和 -1，小矩形中的数字代表单元的类。（b）表示特征空间划分后，特征给定的条件下类的条件概率分布，对应于（a）的划分。当某个单元 c 的条件概率满足 $P(Y=+1|X=c)>0.5$ 时，则认为这个单元属于正类。（c）为对应于（b）中条件概率分布的决策树

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210319165017516.png" alt="image-20210319165017516" style="zoom:50%;" />

## 决策树学习

给定训练数据集 $D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}，x_i=\{x_i^1,x_i^2,\cdots,x_i^n\}^T为输入实例（特征向量），y_i\in\{1,2,\cdots,K\}$，学习的目标是根据训练数据集构建一个决策树模型，使它能够对实例进行正确的分类

决策树学习的本质是从训练数据集中总结出一套分类规则，于训练数据集不相矛盾（即能够对训练数据集正确分类）的决策树可能有多个，也可能一个也没有。我们需要的是一个与训练数据集矛盾较小且具有很好的泛化能力的决策树。从另一个角度看就是由训练数据集估计条件概率模型，基于特征空间划分的类的条件概率模型由无穷个。

决策树学习用损失函数表示这一目标，通常采用正则化的极大似然函数，其策略是以损失函数为目标函数的最小化

此后，学习问题就变成了损失函数意义下选择最优决策树的问题了。从所有可能的决策树中选择最优决策树是NP 完全问题，所以现实中决策树学习算法通常采用启发式算法，近似求解这一最优化问题，这样得到的决策树是次最优（sub-optimal）的

决策树学习的算法通常是递归的选择最优特征，并根据该特征对训练数据进行分割，使得的各个子数据集有一个最后的分类的过程，这一过程对应着特征空间的划分也对应着决策树的构建。

开始，构建根结点，将所有数据都放在根结点，选择一个最优特征，按照这一特征将数据集分割成子集，使各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本分类正确，那么构建叶结点，并将这些子集放到对应的叶结点中，如果，还有的子集不能被正确分类，那么就对这些子集选择新的最优特征，继续分割，如此递归下去，直到所有训练子集被基本分类正确，或者没有合适的特征为止，最后每个子集都被放到合适的叶结点，即有了明确的类别，就生成了一个决策树

以上方法可能对训练数据集有很好的分类效果，但是容易过拟合，因此需要剪枝处理。

如果特征数量很多，可以在学习开始的时候，进行特征的选择，只留下对训练数据有足够分类能力的特征

依次，决策树算法包括特征选择，决策树的生成，决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树代表了不同复杂度的模型，决策树的生成对应于模型的局部选择，剪枝对应于模型的全局选择。

# 特征选择

特征选择在于选取对训练数据集有分类能力的特征，这样可以提高决策树学习的效率。如果利用一个特征分类的结果和随机分类的结果相差不到，则认为该特征没有分类能力，经验上丢掉这样的特征对决策树学习的精度影响不大。特征选择的准则是信息增益或信息增益比

例：下表为贷款申请训练数据，数据包括贷款申请人的 4 个特征，年龄：青年，中年，老年；有无工作：是，否；有无房子：是，否；信贷情况：非常好，好，一般。

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210319180309780.png" alt="image-20210319180309780" style="zoom:50%;" />

下图表示从上表数据学习到的两个可能的决策树，分别由两个不同特征的根结点组成。两个决策树都可以由此延续下去，问题是：哪个更好些，这就要求确定选择特征的准则，直观上，如果一个特征拥有更好的分类能力，那就应该选择这个准则，信息增益（information gain）能够很好的表示这一直观的准则

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210319180800498.png" alt="image-20210319180800498" style="zoom:50%;" />

## 信息增益

在信息论和概率统计中，熵（entropy）是表示随机变量不确定性的度量，设 X 是一个取有限个值的离散随机变量，其概率分布为 
$$
P(X=x_i) = p_i
$$
则随机变量的熵定义为
$$
H(X) = -\sum p_ilog\ p_i
$$
若 $p_i=0,0log0=0$，上式中对数通常以 2 或 e 为底，称为比特（bit）或纳特（nat），由定义可知，熵只与 X 的分布有关而与值无关，所以也可以记作 H(p)

熵越大，随机变量的不确定性越大。从定义可验证
$$
0 <=H(p)<=log\ n
$$
随机变量只取两个值，例如 0 和 1 时，X 的分布为
$$
H(X = 1) = p,H(X=0)=1-p,0<=p<=1
$$
熵为
$$
H(X) = -plog\ p-(1-p)log\ (1-p)
$$
此时，熵随概率的变化曲线如下。

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210322154542150.png" alt="image-20210322154542150" style="zoom:50%;" />

当 p=0 或 p=1 时，熵为 0，没有不确定性，当 p=0.5 时，熵最大，随机变量的不确定性也最大

设有随机变量（X，Y），联合概率分布为
$$
P(X=x_i,Y=y_j) = p_{ij}
$$
条件熵 H(Y|X) 表示 X 确定的条件下随机变量 Y 的不确定性，H(Y|X) 定义为 X 给定条件下随机变量 Y 的条件概率分布的熵对 X 的数学期望
$$
H(Y|X) = -\sum p_i H(Y|X=x_i)\qquad p_i=P(X=x_i)
$$
当熵和条件熵中的概率由数据估计（尤其是极大似然估计）得到，则分别被称为经验熵（empirical）和经验条件熵（empirical conditional）

信息增益（information gain）表示得知特征 X 的信息而使得类 Y 的信息的不确定性减少的程度

**定义**

特征 A 对训练数据集 D 的信息增益 g(D,A)，定义为集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 经验条件熵 H(D|A)之差
$$
g(D,A) = H(D)-H(D|A)
$$
这个差值也被称为互信息（mutual）

根据信息增益的特征选择方法是：对训练数据集 D，计算其每个特征的信息增益，选择信息增益最大的特征

训练数据集 D，|D| 为其样本容量，设有 K 个类 $C_k$，$|C_k|$ 为每个类的样本容量，$\sum_{i=1}^K |C_k| = |D|$，特征 A 有 n 个不同的取值 $\{a_1,a_2,\cdots,a_n\}$，根据特征 A 的取值将 D 分为 n 个子集 $D_i$，$|D_i|$ 为每个子集的样本个数，记子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D_{ik}$，即 $D_{ik} = D_i\cap C_k$，$|D_{ik}|$ 为其样本个数

**算法5.1**

输入：数据集 D 和特征 A

输出：特征 A 对数据集 D 的信息增益 g(D,A)

（1）计算数据集 D 的经验熵 H(D)
$$
H(D) = -\sum \frac{|C_k|}{|D|}log\frac{|C_k|}{|D|}
$$
（2）计算 A 对 D 的经验条件熵 H(D|A)
$$
H(D|A) = \sum \frac{|D_i|}{|D|}H(D_i) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}log\frac{|D_{ik}|}{|D_i|}
$$
（3）计算信息增益
$$
g(D,A) = H(D)-H(D|A)
$$
例：对上表所给的数据集 D，根据信息增益准则选择最优特征

（1）计算经验熵
$$
H(D) = -[(\frac{9}{15})log(\frac{9}{15})+(\frac{6}{15})log(\frac{6}{15})]=0.971
$$
（2）计算条件熵

特征 A 由 4 个值组成，A1 为年龄，A2 为工作，A3 为房子，A4 为信贷情况，分布计算条件熵及信息增益
$$
H(D|A_1) = -[\frac{5}{15}H(D_1) + \frac{5}{15}H(D_2) + \frac{5}{15}H(D_3)]\\
=-[\frac{5}{15}(\frac{2}{5}log\frac{2}{5} + \frac{3}{5}log\frac{3}{5})+\\
\frac{5}{15}(\frac{2}{5}log\frac{2}{5} + \frac{3}{5}log\frac{3}{5}) +\\
\frac{5}{15}(\frac{1}{5}log\frac{1}{5} + \frac{4}{5}log\frac{4}{5})] = 0.888
\\g(D,A_1) = H(D) - H(D|A_1) = 0.083
$$
这里的 D1，D2，D3 分别为年龄是青年，中年，老年的样本子集，类似地
$$
g(D,A_2) = H(D) - [\frac{5}{15}H(D_1) + \frac{10}{15}H(D_2)] = 0.324\\
g(D,A_3) = H(D) - [\frac{6}{15}H(D_1) + \frac{9}{15}H(D_2)] = 0.402\\
g(D,A_4) = H(D) - [\frac{5}{15}H(D_1) + \frac{6}{15}H(D_2) + \frac{4}{15}H(D_3)] = 0.363
$$
根据上述结果可知，特征 A3 的信息增益最大，则选择特征 A3 作为最优特征

## 信息增益比-C4.5

信息增益值的大小是相对于训练数据集而言的，并没有绝对意义，在分类困难时，即数据集的经验熵大时，信息增益值会偏大，否则会偏小，使用信息增益比（ratio）可以对这一问题进行矫正

**定义**

特征 A 对数据集 D 的信息增益比 $g_R(D,A)$，定义为其信息增益 g(D,A) 与 D 的经验熵之比
$$
g_R(D,A) = \frac{g(D,A)}{H(D)}
$$

ID3 无法处理连续值，C4.5 可以，在处理连续值时，将连续值划分成 m 个部分，取相邻两部分的均值作为切分点，计算信息增益，选信息增益（这里如果采用增益比会影响信息度量的准确性）大的作为最优切分点。

# 决策树的生成

## ID3

该算法的核心是决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体地：从根结点开始，计算所有可能的特征的信息增益，选择信息增益最大的作为该结点的特征，由该结点的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树。直到所有特征的信息增益均很小或者没有特征可以选择为止。最后得到一棵决策树。该方法相当于极大似然法进行概率模型的选择

**算法5.2**

输入：数据集 D，特征 A，阈值 $\xi$

输出：决策树 T

（1）若 D 中所有实例均属于同一类 $C_k$，则 T 为单结点树，以类 $C_k$ 作为该结点的类标记，返回 T；

（2）若 $A=\empty$，则 T 为单结点数，将 D 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 T；

（3）否则，按算法 5.1 计算 A 中各特征对 D 的信息增益，选择信息增益最大的特征 $A_g$；

（4）如果 $g(D,A_g)<\xi$ ，则 T 为单结点树，将 D 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 T；

（5）否则，对 $A_g$ 的每一个可能值 $a_i$，将 D 分割成若干非空子集 $D_i$，将 $D_i$ 中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树 T，返回 T；

（6）对第 i 个子结点，以 $D_i$ 为数据集，以 $A-\{A_g\}$ 为特征集，递归地调用 1~5，得到子树 $T_i$，返回。

例：以上表的数据集利用该算法建立决策树

由于 A3 的信息增益最大，以特征 A3 作为根结点的特征，它将 D 分为两个子集 D1（A3 取值为 “是” ）和 D2（A3 取值为 “否” ），由于 D1 只含有同一类样本，所以他为一个叶结点，结点的类标记为 “是”

对 D2，则从特征 A1，A2，A4 中选择新的特征，计算信息增益
$$
g(D_2,A_1)=H(D_2)-H(D_2|A_1)\\
=-\sum_{k=1}^K\frac{|C_k|}{|D_2|}log\frac{|C_k|}{|D_2|}+\sum_{i=1}^n\frac{|D_2^i|}{|D_2|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_2^i|}log\frac{|D_{ik}|}{|D_2^i|}\\
=[\frac{3}{9}log\frac{3}{9} + \frac{6}{9}log\frac{6}{9}]-[\frac{4}{9}(\frac{3}{4}log\frac{3}{4} + \frac{1}{4}log\frac{1}{4}) + \frac{2}{9}*0 + \frac{3}{9}(\frac{1}{3}log\frac{1}{3} + \frac{2}{3}log\frac{2}{3})]\\
=0.918-0.667=0.251\\
g(D_2,A_2)=H(D_2)-H(D_2|A_2)=0.918\\
g(D_2,A_4)=H(D_2)-H(D_2|A_4)=0.474
$$
选择信息增益最大的 A2 作为结点的特征，由于 A2 有两个取值，因此引出两个子结点：“是” 子结点，包含 3 个样本，属于同一类，所以是一个叶结点；“否” 子结点，包含 6 个样本，属于同一类，所以是一个叶结点。

生成如下决策树

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210322184027963.png" alt="image-20210322184027963" style="zoom:50%;" />

该算法只有树的生成，容易过拟合

## C4.5

与 ID3 相似，只是特征选择采用信息增益比准则

# 剪枝

决策树算法容易产生过拟合现象，原因在于生成过程中过于注重如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，解决方法就是考虑决策树的复杂度，对已生成的决策树进行简化，这个过程就称为剪枝（pruning）。具体地：从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型

决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数（cost）来实现，设决策树 T 的叶结点数为 |T|，t 是 T 的叶结点，t 上有 $N_t$ 个样本，k 类的样本点有 $N_{tk}$ 个，$H_t(T)$ 为叶结点 t 上的经验熵，$\alpha >= 0$ 为参数，损失函数可定义为
$$
C_{\alpha}(T) =-\sum_t N_{t}H_t(T) + \alpha|T|\\ 
=-\sum_t N_{t}\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t} + \alpha|T|\\
=-\sum_t\sum_k N_{tk}\ log\frac{N_{tk}}{N_t} + \alpha|T|
$$
损失函数第一项记作 C(T)，表示模型对训练数据的预测误差，即拟合程度，|T| 表示模型的复杂程度，$\alpha$ 控制两者的影响，大的 $\alpha$ 促使选择较简单的模型，反之选择复杂的模型，若为 0 则表示不考虑模型的复杂度

剪枝，就是 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树，$\alpha$ 确定时，子树越大，与训练数据的拟合往往越好，但是模型的复杂度越高，反之亦然，损失函数很好的反映了两者的平衡

决策树的生成只考虑了通过提高信息增益或增益比对训练数据进行更好的拟合，而剪枝通过优化损失函数还考虑了减小模型复杂度，决策树生成学习局部的模型，剪枝则学习整体的模型

该损失函数的极小化等价于正则化的极大似然估计

**算法5.4**

输入：树 T，参数 $\alpha$

输出：剪枝后的子树 $T_{\alpha}$

（1）计算每个结点的经验熵

（2）递归地从叶结点向上回缩

设一组叶结点回缩到父结点之前与之后的整体树分别为 $T_B、T_A$，损失函数值分别为 $C_{\alpha}(T_B)、C_{\alpha}(T_A)$，若

$C_{\alpha}(T_B) >=C_{\alpha}(T_A)$，则进行剪枝，将其父结点变为新的叶结点

（3）返回（2），直至不能继续为止，得到损失函数最小的子树

上式只考虑两棵树的损失函数之差，计算只在局部进行，所以可以利用动态规划算法实现。

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210322193832381.png" alt="image-20210322193832381" style="zoom:50%;" />

# CART算法

分类与回归树（classification and regression，CART）同样由特征选择，树的生成，树的剪枝组成

CART 是在给定随机变量 X 的条件下输出随机变量 Y 的条件概率分布的学习方法。它假设决策树是二叉树，内部节点的特征取值为 “是” 和 “否”，左分支为 “是”，右分支为 “否”，这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

算法由以下两步组成：

（1）生成：基于训练数据集生成决策树，树要尽量大

（2）剪枝：用验证数据集对生成的树剪枝并选择最优子树，此时用损失函数最小化作为剪枝的标准

## 生成

决策树的生成就是递归地构建二叉决策树的过程，对回归树采用平方误差最小化准则，对分类树采用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树

### 回归树的生成

假设已将输入空间划分成 M 个单元 $R_1,R_2,\cdots,R_M$，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$，此时，可以用平方误差 $\sum_{x_i\in R_m}(y_i-f(x_i))^2$ 来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值	

**算法5.5（最小二乘回归树）**

输入：训练数据集 D

输出：回归树 f(x)

（1）采用启发式方法，选择第 j 个变量 $x^j$ 和它取的值 s，作为切分变量（splitting variable）和切分点（splitting point），对固定的输入变量 j 可以找到最优切分点 s，遍历所有输入变量，找到最优的切分变量 j，构成一对 （j，s），具体地，用选定的（j，s）划分区域并决定输出值：
$$
R_1(j,s) = \{x|x^j>=s\},R_2(j,s)=\{x|x^j<s\}\\
\hat {c_m}=\frac{1}{N}\sum_{x_i\in R_m}y_i,m=1,2\\
求解：\\
min_{j,s}\{min_{c_1}\sum_{x_i\in R_1}(y_i-c_1)^2 + min_{c_2}\sum_{x_i\in R_2}(y_i-c_2)^2\}
$$
选择使上式值达到最小的对（j，s），依次将输入空间划分为两个区域，接着对每个区域重复上述划分过程，直到满足停止条件为止。

（2）将输入空间划分成 M 个区域 $R_1,R_2,\cdots,R_M$，生成决策树
$$
f(x) = \sum_{m=1}^M\hat c_mI(x\in R_m)
$$

### 分类树的生成

分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点

**基尼指数定义：**假设有 K 个类，样本点属于第 k 类的概率为 $p_k$，则概率分布的基尼指数定义为
$$
Gini(p)=\sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2
$$
对于二分类问题，样本属于第一个类的概率为 p，基尼指数为
$$
Gini(p) = 2p(1-p)
$$
对给定的样本集合 D，基尼指数为
$$
Gini(p)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2
$$
$C_k$ 是 D 中属于第 k 类的样本子集，K 为类的个数

如果样本集合 D 根据特征 A 是否取某一可能值 a 被分割成 $D_1,D_2$ 两部分，即
$$
D_1=\{(x,y)\in D|A(x)=a\},D_2=D-D_1
$$
则在特征 A 的条件下，D 的基尼指数为
$$
Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)
$$
Gini(D) 表示集合 D 的不确定性，Gini(D,A) 表示经特征 A=a 分割后集合 D 的不确定性，基尼指数越大，不确定性越大

信息增益和基尼指数相比，存在对数运算，时间复杂度大。

下图为Gini(p)，熵之半 $\frac{1}{2}H(p)$ 和分类误差率之间的关系，可以看出基尼指数和熵之半的曲线很接近，可以近似的代表分类误差率

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210324103431350.png" alt="image-20210324103431350" style="zoom:50%;" />

**算法5.6-分类树生成算法**

输入：D，停止计算的条件

输出：CART决策树

根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树

（1）对数据集 D，利用现有特征计算基尼指数。对每个特征 A，及它的每个取值 a，根据样本点对 A=a 的测试为 “是” 或 “否”，分为 D1 和 D2 两部分，计算 A=a 时的基尼指数

（2）对所有特征 A 和所有可能的切分点 a，选择基尼指数最小的作为最优切分特征和切分点，从现结点生成两个子结点，将数据集依特征分到两个子结点中

（3）对两个子结点递归地执行（1）-（2）两步，直到满足停止条件

（4）生成 CART 决策树

算法停止条件是结点中样本少于阈值，或样本集的基尼指数小于阈值（样本基本属于同一类），或者没有更多特征

例：用上表数据集，建立 CART 决策树

首先计算各特征的基尼指数，选择最优特征和最优切分点

特征 A1，
$$
Gini(D,A_1=1)=\frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)\\
=\frac{|D_1|}{|D|}2p_1(1-p_1) + \frac{|D_2|}{|D|}2p_2(1-p_2)\\
=\frac{|D_1|}{|D|}2*\frac{|D_{1k}|}{|D_1|}(1-\frac{|D_{1k}|}{|D_1|}) + \frac{|D_2|}{|D|}2*\frac{|D_{2k}|}{|D_2|}(1-\frac{|D_{2k}|}{|D_2|})\\
=\frac{5}{15}(2*\frac{2}{5}*(1-\frac{2}{5})) + \frac{10}{15}(2*\frac{7}{10}*(1-\frac{7}{10}))
=0.44\\
Gini(D,A_1=2)=0.48\\
Gini(D,A_1=3)=0.44
$$
由于 A1=1 和 A1=3 相等，所以都可以作为 A1 的最优切分点。
$$
Gini(D,A_2=1)=0.32\\
Gini(D,A_3=1)=0.27
$$
A2 和 A3 都只有一个切分点，即为最优切分点
$$
Gini(D,A_4=1)=0.36\\
Gini(D,A_4=2)=0.47\\
Gini(D,A_4=3)=0.32
$$
A4=3 为 A4 的最优切分点

在所有特征中，A3=1 的基尼指数最小，所以选择该特征为最优特征，A3=1 为最优切分点，于是根结点生成两个子结点，一个是叶结点，另一个继续使用上述方法在 A1，A2，A4 中选择最优切分特征及最优切分点，结果是 A2=1，依次计算得所有结点均为叶结点。所生成的树与按照 ID3 所生成的树完全一致。

## CART剪枝

该算法由两步组成：首先从生成算法产生的决策树 T0 底端开始不断剪枝，直到根结点，形成一个子树序列 $\{T_0,T_1,\cdots,T_n\}$，然后通过交叉验证法利用独立的验证数据集上对子树序列进行测试，选择出最优子树

（1）剪枝，形成一个子树序列

剪枝过程中，计算子树的损失函数
$$
C_{\alpha}(T) = C(T) + \alpha|T|\\
C(T) = \sum_tN_t(1-\sum_k(\frac{N_{tk}}{N_t})^2)
$$
T 为任意子树，C(T) 为对训练数据的预测误差，|T| 为子树的叶结点数，$\alpha>=0$ 为参数，$C_{\alpha}(T)$ 为参数是 $\alpha$ 时子树 T 的整体损失，对固定的 $\alpha$ 一定存在使损失函数最小的子树 $T_{\alpha}$，即为最优子树。容易验证这样的子树是唯一的，当 $\alpha$ 大时，最优子树 $T_{\alpha}$ 偏小，反之亦然。极端情况，当 $\alpha=0$ 时，整棵树是最优点，当 $\alpha\rightarrow\infty$ 时，根结点组成的单结点树是最优的

Breiman等人证明：可以用递归的方式对树进行剪枝，将 $\alpha$ 从小增大，$0<\alpha_0<\alpha_1<\cdots<\alpha_n<+$，产生一系列的区间 $[\alpha_i,\alpha_{i+1})，i=1,2,\dots,n$，剪枝得到的子树序列对应着区间 $\alpha\in [\alpha_i,\alpha_{i+1})$ 的最优子树序列，序列中的子树是嵌套的。

具体地，从 T0 开始剪枝，对 T0 任意内部结点 t，以 t 为单结点树的损失函数为
$$
C_{\alpha}(t) = C(t) + \alpha
$$
以 t 为根结点的子树的损失函数为
$$
C_{\alpha}(T_t) = C(T_t) + \alpha|T_t|
$$
 $\alpha=0及\alpha$ 充分小时，有不等式
$$
C_{\alpha}(T_t) <C_{\alpha}(t)
$$
 $\alpha$ 增大时，在某一 $\alpha$ 有
$$
C_{\alpha}(T_t)=C_{\alpha}(t)
$$
再增大时，不等式反向，只要
$$
\alpha=\frac{C_{\alpha}(t)-C_{\alpha}(T_t)}{|T_t|-1}
$$
$T_t$ 和 t 有相同的损失，而 t 的结点少，因此更可取，对 $T_t$ 剪枝

为此，对 $T_0$ 每一个内部结点 t，计算
$$
g(t) =\frac{C_{\alpha}(t)-C_{\alpha}(T_t)}{|T_t|-1}
$$
它表示剪枝后损失函数的减少程度，减去 g(t) 最小的 $T_t$，将得到的子树作为 $T_1$，将最小的 g(t) 作为 $\alpha_1$，T1 为区间 $[\alpha_1,\alpha_2)$ 之间最优的子树。如此剪枝下去，直到得到根结点，再这一过程中，不断增加 $\alpha$ 的值，产生新的区间

（2）在剪枝得到的子树序列中通过交叉验证选取最优子树 $T_{\alpha}$

利用独立的验证数据集，测试子树序列中各个子树的平方误差或基尼指数，平方误差或基尼指数最小的被认为是最优决策树，每个子树 $T_1,T_2,\cdots,T_n$ 都对应着一个参数 $\alpha_1,\alpha_2,\cdots,\alpha_n$，当最优子树 $T_k$ 确定时，$\alpha_k$ 也就确定了，即得到了最优决策树 $T_{\alpha}$

**算法5.7-剪枝**

输入：生成算法产生的决策树 T0

输出：最优决策树 $T_{\alpha}$

（1）设 k=0，T=T0

（2）$\alpha=+\infty$

（3）自下而上对各内部节点 t 计算 $C(T_t),|T_t|$ 以及
$$
g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}\\
\alpha = min(\alpha,g(t))
$$
（4）自上而下访问内部节点 t，如果有 $g(t)=\alpha$，进行剪枝，并对叶结点 t 以多数表决法决定其类，得到树 T

（5）设 $k=k+1，\alpha_k=\alpha，T_k=T$

（6）如果 T 不是由根结点单独组成的树，则回到步骤（4）

（7）采用交叉验证法，选取最优子树 $T_{\alpha}$

























