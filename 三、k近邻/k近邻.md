k-近邻法（k-nearest neighbor，kNN）是一种基本分类和回归算法，输入为实例的特征向量，对应于特征空间的点，输出为实例的类别，可以取多类。k 近邻法假设给定一个训练数据集，其中的实例类别已定，分类时，对新的实例，根据 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。因此，k 近邻不具有显式的学习过程，k 近邻实际上利用训练数据集对特征向量空间进行划分，将其作为分类的模型。k 值的选择，距离度量及分类决策规则是三要素

# 算法

输入：数据集 T，实例特征向量 x

输出：实例 x 所属的类 y

1. 根据给定的距离度量，在训练集 T 中找出与 x 最邻近的 k 个点，涵盖这 k 个点的邻域称为 $N_k(x)$
2. 在 $N_k(x)$ 中根据分类决策规则（如多数表决）觉得 x 的类别 y

$$
y = argmax_{c_j}\sum _{x_i\in N_k(x)}I(y_i=c_j)
$$

​			$I$ 为指示函数，即当 $y_i = c_j$ 时为 1，否则为 0

k 近邻的特殊情况是 k=1，称为最近邻算法，对应输入的实例点 x，最近邻法将训练数据集中与 x 最邻近点的类作为 x 的类

# 模型

k 近邻法使用的模型实际上对应于特征空间的划分，模型由三要素决定。

## 距离度量

特征空间中两个实例点的距离反映了两个点的相似程度，k 近邻模型的特征空间一般是 n 维实数向量空间，距离一般是 $L_p$ 距离或 Minkowski 距离

设特征空间 X 是 n 维实数向量空间 $R^n$，$x_i,x_j \in X，x_i = (x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T，x_j= (x_j^{(1)},x_j^{(2)},\dots,x_j^{(n)})^T$，$x_i,x_j$ 的 $L_P$ 距离为
$$
L_p(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^p)^{\frac1p}
$$
当 p=2 时，为欧式距离，p=1 时为曼哈顿距离

$p=\infty$ 时，它是各个坐标距离的最大值
$$
L_\infty(x_i,x_j) = max_l|x_i^{(l)} - x_j^{(l)}|
$$
下图给出二维空间中 p 取不同值，与原点的 $L_p$ 距离为 1($L_p=1$) 的点的图像

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210207110812557.png" alt="image-20210207110812557" style="zoom:50%;" />

下面的例子说明在 p 不同时，最近邻点是不同的

例：已知二维空间的三个点 $x_1=(1,1)^T,x_2=(5,1)^T,x_3=(4,4)^T$，试求 p 不同时，$L_p$ 距离下，$x_1$ 的最近邻点

$x_1,x_2$ 只有第二维上不同，因此 $L_p(x_1,x_2) = 4$，而 $L_1(x_1,x_3) = 6，L_2(x_1,x_3) = 4.24,L_3(x_1,x_3)=3.78,L_4(x_1,x_3)=3.57$，于是有 p=1,2 时，x2是 x1 的最近邻点，p >= 3 时，x3 是 x1 的最近邻点

## k 值的选择

如果选择较小的 k 值，就相当于用较小的邻域内的训练实例进行预测，学习的近似误差（approximation error）会减小，只有与输入实例较近（相似）的训练实例才会对预测结果有作用，缺点是学习的估计误差（estimation）会增大，预测结果对近邻的实例点非常敏感，如果邻近的点恰好是噪声，预测就会出错。也就是说，k 值的减小意味着整体模型变得复杂，容易过拟合

如果选择较大的 k 值，就相当于用较大的邻域内的训练实例进行预测，相应的估计误差会减小，但是近似误差会增大，这时与输入实例较远的（不相似）的点也会对预测起作用，使预测发生错误，k 值的增大会使模型变简单

在应用中，一般会将 k 取一个较小的值，然后采用交叉验证法来选取最优的 k 值

## 分类决策规则

k 近邻法中的分类规则往往采用多数表决，即有输入实例的 k 个邻近的训练实例中的多数类决定输入实例的类别。

多数表决规则（majority voting rule）：如果分类的损失函数为 0-1 损失函数，分类函数为
$$
f:R^n \rightarrow \{c_1,c_2,\dots,c_k\}
$$
那么误分类的概率为
$$
P(Y\neq f(X)) = 1 - P(Y=f(X))
$$
对于给定的实例 x，其最邻近的 k 个训练实例点构成集合 $N_k(x)$，如果涵盖 $N_k(x)$ 区域的类别为 $c_j$，那么误分类率为
$$
\frac1k\sum_{x_i\in N_k(x)}I(y_i\neq c_j) = 1 - \frac1k\sum_{x_i\in N_k(x)}I(y_i = c_j)
$$
要是误分类率最小即经验风险小，则要 $\sum_{x_i\in N_k(x)}I(y_i = c_j)$ 最大，所以多数表决规则相当于经验风险最小化



# k 近邻法的实现：kd树

实现 k 近邻时主要考虑的是如何对训练数据进行快速 k 近邻搜索，这点在特征空间的维数大及训练数据容量大时尤为重要

k 近邻最简单的方法是线性扫描（linear scan），但是这种方法要计算实例点到每个训练数据的距离，当数据量大时，计算非常耗时

为了提高 k 近邻搜索的效率，可以考虑特殊的结构存储训练数据，以减少计算距离的次数，下面介绍 kd树（kd tree）方法

## 构造kd树

kd 树是一种对 k 维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。kd 树是二叉树，表示对 k 维空间的一个划分(partition)。构造 kd 树相当于不断地用垂直于坐标轴的超平面将 k 维空间切分，构成一系列的 k 维超矩形区域。kd 树的没给结点对应于一个 k 维超矩形区域

构造方法如下：构造根节点，使根节点对应于 k 维空间中包含所有实例点的超矩形区域；通过递归，不断地对 k 维空间进行切分，生成子结点。在超矩形区域（结点）上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时实例被分到两个子区域。这个过程直到子区域内没有实例时终止（终止时的结点为叶结点）。在此过程中，将实例保存在相应的结点上

通常依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数为切分点，这样的 kd 树是平衡的，但搜索时的效率未必是最优的

### 算法——构造平衡 kd 树

输入：k 维空间数据集 T，其中 $x_i = (x_i^{(1)}，x_i^{(2)}，\cdots，x_i^{(k)})^T$

输出：kd 树

（1）开始：构造根结点，根结点对应于包含 T 的 k 维空间的超矩形区域

选择 $x^{(1)}$ 为坐标轴，以 T 中所有实例的 $x^{(1)}$ 坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域，切分由通过切分点并与坐标轴 $x^{(1)}$ 垂直的超平面实现

由根结点生成深度为 1 的左右子结点：左子结点对应坐标  $x^{(1)}$ 小于切分点的区域，右子结点对应于坐标 $x^{(1)}$ 大于切分点的子区域

将落在切分超平面上的实例点保存在根结点

（2）重复：对深度为 j 的结点，选择  $x^{(l)}$ 为切分的坐标轴，$l = j(mod\ k) + 1$，以该结点的区域中所有实例的  $x^{(l)}$ 坐标的中位数为切分点进行切分

由该结点生成深度为 j + 1 的左右子结点，将落在切分超平面上的实例点保存在该结点

例：给定一个二维空间的数据集：
$$
T = \{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}
$$
构造平衡树

解：根结点对应包含数据集 T 的矩形，选择  $x^{(1)}$ 轴，6个数据点的 $x^{(1)}$ 坐标的中位数是 7，以平面  $x^{(1)}$ = 7 将空间分为左右两个子矩形（子结点）；接着，左矩形以  $x^{(2)}$ = 4 分为两个子矩形，右矩形以 $x^{(1)}$ = 6 分为两个子矩形，如此递归，最后得到如图所示的特征空间划分和 kd 树

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210310151958450.png" alt="image-20210310151958450" style="zoom:50%;" />

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210310152023276.png" alt="image-20210310152023276" style="zoom:50%;" />

### 搜索 kd 树

下面介绍如何利用 kd 树进行 k 近邻搜索，可以看到，利用 kd 树可以省去对大部分数据点的搜索，从而减少搜索的计算量，这里以最近邻为例加以叙述，同样的方法可以应用到 k 近邻

给定一个目标点，搜索其最近邻，首先找到包含目标点的叶结点；然后从该叶结点出发，依次回退到父结点；不断查找与目标点最邻近的结点，当确定不可能存在更近的结点时终止，这样搜索就被限制在空间的局部区域上，效率大为提高

包含目标点的叶结点对应包含目标点的最小超矩形区域，以此叶结点的实例点作为当前最近点，目标点的最近邻一定在以目标点为中心并通过当前最近点的超球体的内部，然后返回当前结点的父结点，如果父结点的另一子结点的超矩形区域与超球体相交，那么在相交的区域内寻找与目标点更近的实例点，如果存在这样的点，将此点作为新的当前最近点，算法到达更上一级的父结点，继续上述过程，如果父结点的另一子结点的超矩形与超球体不相交，或者不存在比当前最近点更近的点，则停止搜索

**算法：用 kd 树的最近邻搜索**

输入：已构造的 kd 树；目标点 x

输出：x 的最近邻

（1）在 kd 树中找出包含目标点 x 的叶结点：从根结点出发，递归地向下访问 kd 树，若目标点 x 当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点，直到子结点为叶结点为止

（2）以此叶结点为 “当前最近点”

（3）递归地向上回退，在每个结点进行以下操作：

​	（a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为 “当前最近点”

​	（b）当前最近点一定存在于该结点一个子结点对应的区域，检查该子结点的父结点的另一子结点对应的区域是否有更近的点，具体地，检查另一子结点对应的区域是否与以目标点为球心，与 “当前最近点”的距离为半径的超球体相交

​	如果相交，可能在另一个子结点对应的区域内存在更近的点，移动到另一子结点，接着，递归地进行最近邻搜索

​	如果不相交，向上回退

（4）当回退到根结点时，搜索结束，最后的 “当前最近点” 即为 x 的最近邻点

如果实例点是随机分布的，kd 树搜索的平均计算复杂度是 $O(log\ N)$，这里的 N 是训练实例数，kd 树更适用于训练实例数远大于空间维数时的 k 近邻搜索，当空间维数接近训练实例数时，它的效率会迅速下降，几乎接近线性扫描

例：给定一个如下图所示的 kd 树，根结点为 A，其子结点为 B，C等，共有 7 个实例点；另有一个输入目标实例点 S，求 S 的最近邻

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210310155427397.png" alt="image-20210310155427397" style="zoom:50%;" />

解：首先在 kd 树中找到包含点 S 的叶结点 D，以点 D 作为近似最近邻，真正最近邻一定在以点 S 为中心通过点 D 的圆的内部。然后返回结点 D 的父结点 B，在结点 B 的另一子结点 F 的区域内搜索最近邻，结点 F 的区域与圆不相交，不可能有最近邻点，继续返回上一级父结点 A，在结点 A 的另一子结点 C 的区域内搜索最近邻，结点 C 的区域与圆相交；该区域在圆内的实例点有点 E，点 E 比点 D 更近，成为新的最近邻点。





![image-20210310162005919](C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210310162005919.png)



