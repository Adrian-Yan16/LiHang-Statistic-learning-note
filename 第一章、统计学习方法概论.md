#    统计学习方法概论

## 统计学习

统计学习是关于计算机基于数据构建概论统计模型，并利用模型对数据进行预测和分析的学科

### 特点

1. 以计算机和网络为平台，是建立在计算机和网络之上的

2. 以数据为研究对象，是数据驱动的学科

   统计学习的对象是数据，从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，最后对数据进行分析预测。数据是多样的，包括文字，图像，音视频等。统计学习的基本假设是同类数据具有一定的统计规律性，所以才可以利用概论统计方法加以处理，比如可以用随机变量描述数据中的特征，用概率分布描述数据的统计规律；统计学习过程中，通常以变量或变量组来表示数据，数据分为连续变量和离散变量

3. 目的是对数据进行预测和分析

   统计学习的目的是对数据进行预测和分析，尤其是对未知数据，对数据进行预测分析可以使计算机更加智能化，或者说使计算机某项功能得到提高；对数据的预测分析是通过构建概率统计模型实现的，统计学习总的目标是考虑学习什么样的模型和如何学习模型，以使模型能对数据更好的预测，同时还要考虑提高学习效率

4. 以方法为中心，统计学习方法构建模型并应用

   统计学习的方法是基于数据构建统计模型从而对数据进行预测分析，分为监督学习，非监督学习，半监督学习，强化学习

   以监督学习为例，统计学习的方法如下：从给定的，有限的，用于学习的训练数据集合出发，假设数据是独立同分布产生的，假设要学习的模型属于某个函数的集合，称为假设空间，应用某个评价标准，从假设空间中选取一个最优模型，使它对已知或未知数据在给定的评价标准下有最好的预测，最优模型的选择由算法实现，这样统计学习的方法包括模型的假设空间，模型选择的准则，模型学习的算法，称其为统计学习方法的三要素，简称模型（model），策略（strategy），算法（algorithm）

5. 是概率论、统计学、最优化理论等多个领域的交叉学科

### 重要性

1. 统计学习是处理海量数据的有效方法，当前时代是一个信息爆炸的时代，因此海量数据的处理是必然的需求，然而现实的数据规模庞大且充满不确定性，统计学习往往是处理这类数据的有力工具
2. 统计学习是实现计算机智能化的重要手段，智能化是计算机发展的必然趋势，也是计算机技术研究与开发的主要目标，利用统计学习方法模仿人类智能的方法，虽有一定的局限性，但仍是实现这一目标的有效手段

## 监督学习

### 基本概念

1. 输入空间，特征空间，输出空间

   输入，输出所有可能的取值分布称为输入空间（input space）和输出空间（output space），每个具体的输入都是一个实例（instance），通常由特征向量（feature vector）表示，所有特征向量所在的空间即为特征空间（feature space），特征空间的每一维对应于一个特征。模型实际上都是定义在特征空间上的

2. 假设空间

   监督学习的目的是学习一个从输入到输出的映射，这一映射由模型表示，学习的目的就在于找到最好的这样的模型，模型属于从输入到输出空间的映射的集合，这个集合就是假设空间（hypothesis space），假设空间的确定意味着学习范围的确定

   监督学习的模型可以是概率模型或非概率模型，由条件概率分布P(Y|X)或决策函数Y=f(X)表示。

3. 联合概率分布

   监督学习假设输入与输出的随机变量 X 和 Y遵循联合概率分布 P(X，Y)，P(X,Y) 表示分布函数或分布密度函数，在学习过程中，假定这样联合概率分布是存在的，但实际上，联合概率分布的具体定义是未知的，训练和测试数据被看作是联合概率分布 P(X,Y) 独立同分布产生的，统计学习假设数据存在一定的统计规律性，X 和 Y 具有联合概率分布的假设是统计学习关于数据的基本假设

### 问题的形式化

监督学习中，假设训练数据和测试数据是由联合概率分布 P(X，Y) 独立同分布产生的

在学习过程中，学习系统利用给定的训练数据，学习或训练得到一个模型，表示为条件概率分布 P(Y|X) 或决策函数 Y = f(X)。

预测过程中，预测系统对于给定的测试样本的输入 $x_{N + 1}$，由模型 $y_{N + 1} = argmax\ p(y_{N+1}|x_{N+1})$ 或决策函数 $y_{N + 1} = f(x_{N + 1})$ 预测出相应的输出 $y_{N + 1}$

在学习过程中，学习系统利用训练数据集 $(x_i,y_i)$ 带来的信息学习模型，具体来说，就是对于一个实例 $x _ i$，模型预测得出一个结果为 $f(x_i)$，而训练集中对应的输出为 $y_i$，如果该模型由较好的预测能力，那模型预测结果 $f(x_i)$ 与训练样本的结果 $y_i$ 之间的差异应该足够的小。学习系统通过不断的学习，选取最好的模型，以便对训练集能有较好的预测，同时也对未知的测试数据集的预测也有尽可能好的推广

## 统计学习三要素

统计学习方法可以表示为
$$
方法 = 模型 + 策略 + 算法
$$

### 模型

统计学习首要考虑的是要学习什么样的模型，监督学习过程中，要学习的模型就是条件概率分布或决策函数，模型的假设空间包含所有的条件概率分布或决策函数，例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合，假设空间中的函数一般由无穷个

假设空间用 $F$ 表示，定义为决策函数的集合
$$
F = \{f| Y = f(X)\}
$$
其中，X 和 Y 是定义在输入空间和输出空间的变量，此时 $F$ 通常是一个参数向量决定的函数族
$$
F = \{f| Y = f_{\theta}(X),\theta\in R^n\}
$$
参数向量 $\theta$ 取值与 n 维欧式空间 $R^n$，称为参数空间

或者定义为条件概率的集合
$$
F = \{P|P(Y|X)\}
$$
此时 $F$ 为一个参数向量决定的条件概率分布族

$$
F = \{P|P_{\theta}(Y|X),\theta \in R^n\}
$$

### 策略

有了模型的假设空间，接着就要考虑使用怎样的准则学习或选择最优模型，统计学习的目标就在从假设空间中选取最优模型

损失函数度量一次预测的好坏，风险函数度量平均意义下模型预测的好坏

#### 1、损失函数和风险函数

监督学习问题是再假设空间 F 中选取模型 f 作为决策函数，对于给定的输入 X，由 f(X) 给出相应的 Y，这个输出与真实值可能一致也可能不一致，用一个损失函数(loss function)或代价函数(cost function)来度量错误的程度，损失函数是 f(X) 和 Y 的非负实值函数，记作 L(Y，f(X))

常用的损失函数如下：

1. 0-1损失函数
   $$
    L(Y，f(X))=
   \begin{cases}
   1， Y \neq f(X)\\
   0, Y = f(X)
   \end{cases}
   $$

2. 平方损失函数（quadratic）
   $$
    L(Y，f(X)) = (Y - f(X))^2
   $$

3. 绝对损失函数（absolute）
   $$
    L(Y，f(X)) = |Y - f(X)|
   $$

4. 对数或对数似然损失函数（logarithmic-likehode）
   $$
   L(Y，P(Y|X)) = -log P(Y|X)
   $$

损失函数值越小，模型就越好，由于模型的输入，输出变量（X，Y）都是随机变量，遵循联合分布 P(X，Y)，所以损失函数的期望是
$$
R_{exp}(f)=E_p[L(Y,f(X))] = \int_{xy}L(y,f(x))P(x,y)dxdy
$$
这是理论上模型 f(X) 关于联合分布 P(X,Y) 的平均意义下的损失，称为风险函数（risk）或期望损失（expected）

学习的目标是选择期望风险最小的模型，由于联合分布 P(X,Y) 是未知的 $R_{exp}(f)$ 不能直接得出，因此需要学习。这样一来，一方面根据期望风险最小学习模型要用到联合分布，另一方面，联合分布又是未知的，因此监督学习就成了一个病态问题（ill-formed problem）。

给定一个数据集
$$
T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
模型 f(X) 关于训练数据集的平均损失称为经验风险（empirical risk）或经验损失，记作$R_{emp}$:
$$
R_{emp}(f) = \frac1N \sum_{l = 1}^N L(y_i,f(x_i))
$$
期望风险是模型关于联合分布的期望损失，经验风险是模型关于训练数据的平均损失，根据大数定律，当样本趋于无穷时，经验风险趋于期望风险，所以可以用经验风险估计期望风险。但是现实中训练样本一般有限，甚至很小，所以估计结果并不理想，要对经验风险进行一定的矫正。

#### 2、经验风险最小化和结构风险最小化

在假设空间、损失函数和训练数据集确定的情况下，经验风险函数 $R_{emp}$ 就可以确定，经验风险最小化（empirical risk minimization，ERM）策略认为经验风险最小的模型就是最优模型。求解：
$$
min_{f\in F}\frac1N L(y_i,f(x_i))
$$
当样本容量足够大时，可以取得很好的学习效果，在现实中被广泛应用，比如极大似然估计（maximum likelihood estimation），当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计

但是当样本容量小时，学习效果就不一定很好，有可能会过拟合（over-fitting）

结构风险最小化（structural，SRM）是为了防止过拟合提出的策略，在假设空间，损失函数和训练数据集确定的情况下，结构风险定义为：
$$
R_{srm}(f) = \frac1N\sum_{i = 1}^N L(y_i,f(x_i)) + \lambda J(f)
$$
其中 J(f) 表示模型的复杂度，是定义在假设空间 F 上的泛函，模型越复杂，J(f) 越大，反之亦然。他代表了对复杂模型的惩罚，$\lambda >= 0$ 用于权衡经验风险和模型复杂度，结构风险小需要经验风险和模型复杂度同时小，结构风险小的模型往往对训练数据和未知数据都有较好的预测结果

贝叶斯估计的最大后验概率（maximum posterior probability estimation）就是结构风险最小化的一个例子，当模型是条件概率模型，损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，两者等价。

这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题，这时，经验或结构风险函数即为最优化的目标函数

### 算法

算法是指学习模型的具体计算方法，统计学习最后需要考虑用什么方法求解最优模型

统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法，如果最优化问题有显示的解，这个问题就比较简单，但通常解析解不存在，这就需要用数值计算的方式求解，如何保证找到全家最优解，并使求解的过程非常高效是一个重要的问题，统计学习可以利用已有的最优化算法，有时也需要开发独自的算法

## 模型评估和模型选择

### 训练误差和测试误差

统计学习的目的是使学到的模型不仅对训练数据还要对测试数据有很好的预测能力。不同的学习方法会给出不同的模型。当损失函数给定时，基于损失函数的模型的训练误差和测试误差就自然成为学习方法评估的标准。

假设学习到的模型为 $Y = \hat f(X)$ ，训练误差是模型关于训练数据集的平均损失
$$
R_{emp}(\hat f) = \frac1N \sum_{i = 1}^N L(y_i,\hat f(x_i))
$$
测试误差为模型关于测试数据集的平均损失
$$
e_{test} = \frac1{N'} \sum_{i = 1}^{N'} L(y_i,\hat f(x_i))
$$
例如，当损失函数为 0-1 损失函数时，测试误差就变成了常见的测试数据集上的误差率（error rate）
$$
e_{test} = \frac1{N'} \sum_{i = 1}^{N'} I(y_i \ne \hat f(x_i))
$$
训练误差的大小，对判断给定的问题是不是一个容易学习的问题是有意义的，测试误差反映了学习方法对未知数据的预测能力，也称为泛化能力（generalization ability）

### 过拟合与模型选择

当假设空间含有不同复杂度（如不同参数个数）的模型时，就要面临模型选择的问题，我们希望选择或学习一个合适的模型，尽量逼近 “真” 模型，具体地，所选择的模型参数个数要与真模型的相同，参数向量相近。

如果一味的追求模型对训练数据的预测能力，往往会过拟合，而对位置数据预测很差，因此模型选择旨在避免过拟合并提高模型的预测能力

下面，以多项式函数拟合问题为例，说明过拟合与模型选择，这是一个回归问题

例：假设给定一个训练数据集
$$
T = \{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
多项式函数拟合的任务是假设给定数据由 M 次多项式函数生成，选择最有可能产生这些数据的 M 次多项式函数，即选择一个对已知和未知数据都有良好预测的函数![image-20210125172850383](C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210125172850383.png)

如上图给定10个数据点，用 0 ~ 9 次多项式函数对数据进行拟合，

设 M 次多项式为
$$
f_M(x,w) = \sum_{j = 1}^M w_jx^j
$$
解决这一问题的方法为，首先确定模型的复杂度，即确定多项式的项数，然后按照经验风险最小化策略，求解参数，即多项式的系数，具体地，求以下经验风险最小化
$$
L(w) = \frac12\sum_{i = 1}^N(f(x_i,w) - y_i)^2
$$
此时损失函数平方损失

这是一个简单的最优化问题，将模型及训练数据带入的
$$
L(w) = \frac12\sum_{i = 1}^N(\sum_{j=0}^Mw_jx_i^j - y_i)^2
$$
对 $w_j$ 求偏导并令其倒数为0，可得
$$
w_j = \frac{\sum_{i = 1}^Nx_iy_i}{\sum_{i = 1}^Nx_i^{j + 1}}
$$
由上图可知，M=9 时多项式曲线通过每个数据点，训练误差为 0，但是因为训练数据本身存在噪声，这种拟合曲线对未知数据的预测能力往往并不好；反而 M = 3 时，多项式函数对训练数据的拟合较好，模型也简单，是较好的选择

下图描述了训练误差，测试误差和模型复杂度的关系，可以看出，当模型的复杂度过大时，会过拟合

![image-20210125174514917](C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210125174514917.png)

## 正则化与交叉验证

### 正则化

模型选择的典型方法是正则化，正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或惩罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如，正则化项是模型参数向量的范数

正则化项可以取不同的形式，例如，回归问题中，损失函数是平方损失，正则化项可以是参数向量的 L2 范数	
$$
L(w) = \frac1N\sum_{i = 1}^N(f(x_i,w) - y_i)^2 + \frac{\lambda}{2}||w||^2
$$
也可以是参数向量的 L1 范数
$$
L(w) = \frac1N\sum_{i = 1}^N(f(x_i,w) - y_i)^2 + \lambda||w||
$$
正则化符号奥卡姆剃刀原理，该原理应用与模型选择时变为以下想法：在所有可能的选择中，模型要能够很好的解释已知数据并且十分简单才是最好的，从贝叶斯的角度看，正则化相当于模型的先验概率，可以假设复杂模型有较大的先验概率，反之亦然

### 交叉验证

如果给定的数据充足，可以将数据随机切分为三份，训练集，验证集，测试集。训练集用于训练模型，验证集用于模型的选择，测试集用于评估模型。

实际中，数据一般不足，因此要采用交叉验证的方式

#### 简单交叉验证

将数据随机的切分为两部分，一部分做训练集，一部分做测试集，然后用训练集在各种情况（如不同参数个数）下训练模型，利用测试数据评估模型，选择误差最小的模型。

#### S折交叉验证

#### 留一交叉验证

S折交叉验证的特殊情况是 S=N，往往是在数据缺乏的情况下使用。

## 泛化能力

泛化能力是指由该方法学习到的模型对未知数据的预测能力，现实中一般用测试数据来评估，但测试数据一般有限，平均结果可能不可靠。因此引入泛化误差。

### 泛化误差

如果学到的模型是 $\hat f$，那么这个模型对未知数据的误差就是泛化误差
$$
R_{exp}(\hat f)=E_p[L(Y,\hat f(X))] = \int_{xy}L(y,\hat f(x))P(x,y)dxdy
$$
事实上，泛化误差就是模型的期望风险

### 泛化误差上界（generalization error bound）

学习方法的泛化能力分析往往是通过研究泛化误差的概率上界进行的。泛化误差上界通常由以下性质，他是样本容量的函数，当样本容量增加时，泛化上界趋于 0，他是假设空间容量的函数，假设空间容量越大，模型就越难学，泛化误差上界就越大

例：二类分类问题的泛化误差上界

考虑二类分类问题，已知数据集 T，他是从概率联合分布 P(X,Y) 独立同分布产生的，假设空间是函数的有限集合 $F = {f_1,f_2,...,f_d}$ ，设 f 是从 F 中选取的函数，损失函数是 0-1 损失，关于 f 的期望风险和经验风险分别为
$$
R(f) = E[L(Y,f(X))]\\
\hat R(f) = \frac1N \sum_{i=1}^d L(y_i,f(x_i))
$$
经验风险最小化函数是
$$
f_d = argmin_{f\in F}\hat R(f)
$$
泛化能力为
$$
R(f_d) = E[L(Y,f_d(X))]
$$
下面讨论从 F 中随机选出的 f 的泛化误差上界

定理：对二类分类问题，从 F 中任意选出的 f，都有概率 $1-\delta$，以下不等式成立
$$
R(f) <= \hat R(f) + \varepsilon(d,N,\delta)\\
\varepsilon(d,N,\delta) = \sqrt{\frac1{2N}(log d + log \frac1{\delta})}\\
\delta = dexp(-2N\varepsilon^2)
$$
不等式左端 R(f) 是泛化误差，右端为泛化误差上界，第一项是训练误差，训练误差越小，泛化误差越小，第二项为 N 的单调递减函数，当 N 趋于无穷时趋于 0，同时他也是 $\sqrt{log d}$ 阶的函数，假设空间 F 中的函数越大，其值越大

证明：证明中用到 Hoeffding 不等式

设 $S_n = \sum_{i = 1}^n X_i$是独立随机变量之和，$X_i\in[a_i,b_i]$，对任意 t>0，以下不等式成立
$$
P(ES_n - S_n >= t) <= exp(\frac{-2t^2}{\sum_{i=1}^n(b_i - a_i)^2})\\
P(S_n - ES_n >= t) <= exp(\frac{-2t^2}{\sum_{i=1}^n(b_i - a_i)^2})\\
$$
对任意 $f\in F$，$\hat R(f)$ 是随机变量 $L(Y,f(X))$ 的样本均值，R(f) 是期望值，如果损失函数取值为 $[0,1]$，那么对于 $\varepsilon > 0$，有
$$
P(R(f) - \hat R(f) >= \varepsilon) <= exp(-2N\varepsilon^2)\\
P(\exist f\in F:R(f) - \hat R(f) >= \varepsilon) = P(\bigcup_{f\in F} R(f) - \hat R(f) >= \varepsilon)\\
<= \sum_{f\in F}P(R(f) - \hat R(f) >= \varepsilon)\\
<= dexp(-2N\varepsilon^2)
$$
或者等价的，对任意 $f\in F$
$$
P(R(f) - \hat R(f) < \varepsilon) >= 1-dexp(-2N\varepsilon^2)\\
令\qquad \delta = dexp(-2N\varepsilon^2)\\
P(R(f) - \hat R(f) < \varepsilon) >= 1-\delta\\
$$
即至少以概率 $1-\delta$ 有 $R(f) < \hat R(f) + \varepsilon$，这就是说训练误差小的模型，泛化误差也会小。

## 生成模型及判别模型

监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出。这个模型的一般形式为决策函数 $Y = f(X)$ 或条件概率分布 $P(Y|X)$

监督学习方法又分为生成学习（generative approach）和判别方法（discriminative）

生成方法由数据学习联合概率分布 $P(X,Y)$，然后求出条件概率分布 $P(Y|X)$ 作为预测模型，即生成模型
$$
P(Y|X) = \frac{P(X,Y)}{P(X)}
$$
之所以称为生成方法，是因为模型表示了给定输入 X 产生输出 Y 的生成关系。典型的有：朴素贝叶斯，隐马尔可夫等

判别方法由数据直接学习出决策函数 f(X) 或条件概率分布 P(Y|X) 作为模型，即判别模型。判别方法关心的是对给定的 X，应该预测什么样的输出。典型的有：k 近邻、感知机、决策树、逻辑回归、最大熵模型、支持向量机、提升方法和条件随机场等

生成方法和判别方法各有优缺点，适用于不同的场景

生成方法：生成方法可以还原出联合概率分布 P(X,Y)，判别方法不能；生成方法的学习收敛速度更快，即当样本容量增加时，生成模型能更快的收敛于真实模型；当存在隐变量时，仍可以用生成方法，不可用判别方法

判别方法：判别方法直接面对预测，准确率往往更高；由于直接学习 f(X) 或 P(Y|X)，可以对数据进行各种程度上的抽象，定义特征并使用特征，因此可以简化学习

## 分类问题

监督学习从数据中学习一个分类模型或分类决策函数，称为分类器（classifier），分类器对新的输入进行预测，称为分类（classification）。

分类问题包括两个过程，在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；分类过程中，利用学习到的分类器对新的输入实例进行分类

评价分类器性能的指标一般是分类准确率：对于给定的测试数据集，分类器分类的正确样本数和总样本数之比。

对二分类问题的常用指标是精确率（precision）和召回率（recall）
$$
P = \frac{TP}{TP + FP}\\
R = \frac{TP}{TP + FN}\\
F_1 = \frac{2TP}{2TP + FP + FN}
$$
常用方法有：k近邻，逻辑回归，支持向量机等

## 标注问题

标注问题的目标在于学习一个模型，使他能对观察序列给出标记序列作为预测，标记的个数可能是有限的，但组合所成的标记序列的个数是依序列长度成指数级增长

给定一个训练数据集 T，$x_i = ({x_i^{(1)},x_i^{(2)},...,x_i^{(n)}})^T,y_i = ({y_i^{(1)},y_i^{(2)},...,y_i^{(n)}})^T$，学习系统基于数据集构建一个模型，表示为条件概率分布
$$
P(Y^{(1)},Y^{(2)},...,Y^{(n)}|X^{(1)},X^{(2)},...,X^{(n)})
$$
这里每一个 X 取值为所有可能的观测，每一个 Y 取值于所有可能的标记。标注系统按照学习得到的条件概率分布模型，对新的输入观测序列找到相应的输出标记序列。具体地，对一个观测序列 $x_i = ({x_i^{(1)},x_i^{(2)},...,x_i^{(n)}})^T$，找出使得条件概率 $P(({y_i^{(1)},y_i^{(2)},...,y_i^{(n)}})^T| ({x_i^{(1)},x_i^{(2)},...,x_i^{(n)}})^T)$ 最大的标记序列 $y_i = ({y_i^{(1)},y_i^{(2)},...,y_i^{(n)}})^T$

标记常用的方法：隐马尔可夫，条件随机场

## 回归问题

回归模型表示从输入到输出的映射函数，回归问题等价于函数拟合：选择一条函数曲线使其很好的拟合已知数据且很好的预测未知数据

回归问题按照输入变量的个数，分为一元回归和多元回归。按照模型的类型分为线性和非线性回归。

回归学习常用的是平方损失函数，这时可利用最小二乘法（least squares）求解


















