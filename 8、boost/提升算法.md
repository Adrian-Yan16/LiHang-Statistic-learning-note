提升（boosting）方法是一种常用的统计学习方法，在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能

# AdaBoost

## 提升方法基本思路

对应分类问题，求比较粗糙的分类规则（即弱分类器）要比求精确的分类规则（强分类器）要容易的多，提升方法就是从弱分类器出发，反复学习，得到一系列弱分类器（基分类器），然后组合这些弱分类器，构成强分类器，大多数提升方法都是改变训练数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器

那么要如何改变训练数据的权值或概率分布以及如何将弱分类器组合成一个强分类器。

AdaBoost 的做法是，提高那些被前一轮弱分类器错误分类的样本的权值，降低那些正确分类样本的权值，这样一来，那些没有得到正确分类的数据，由于权值的加大而受到后一轮弱分类器更大的关注，于是，分类问题被一系列弱分类器“分而治之”；至于弱分类器的组合，AdaBoost 采取加权多数表决的方法。具体地，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用

## 算法

给定二分类训练数据集 T

（1）初始化训练数据的权值分布
$$
D_1 = (w_{11},\cdots,w_{1i},\cdots,w_{iN}),w_{1i} = \frac1N
$$
（2）对 m = 1,2,...,M（此处从 1 - M 按顺序执行）

​	（a）使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器
$$
G_m(x)
$$
​	（b）计算 $G_m(x)$ 在训练数据集上的分类误差率
$$
e_m = P(G_m(x_i)\neq y_i) = \sum_iw_{mi}I(G_m(x_i)\neq y_1)=\sum_{G_m(x_i)\neq y_i}w_{mi}\tag{8.1}
$$
​	（c）计算 $G_m(x)$ 的系数
$$
\alpha_m = \frac12ln\frac{1-e_m}{e_m}\tag{8.2}
$$
​	$\alpha_m$ 表示 $G_m(x)$ 在最终分类器中的重要性，当 $e_m\leq\frac12$，$\alpha_m\geq0$，并且会随着 $e_m$ 的减小而增大，所以分类误差率越小的基分类器在最终分类器中的作用越大

推导
$$
Z_m = \sum_iw_{mi}exp(-\alpha_my_iG_m(x_i))\\
=\sum_{G_m({x_i})\neq y_i}w_{mi}e^{\alpha_m} + \sum_{G_m({x_i})= y_i}w_{mi}e^{-\alpha_m}\\
=e^{\alpha_m}\sum_{G_m({x_i})\neq y_i}w_{mi} + e^{-\alpha_m}\sum_{G_m({x_i})= y_i}w_{mi}\\
=e^{\alpha_m}e_m + e^{-\alpha_m}(1-e_m)\\
令\ \frac{\partial Z_m}{\partial \alpha_m} = 0\\
\rightarrow e^{2\alpha_m} = \frac{1-e_m}{e_m}\\
\rightarrow \alpha_m = \frac12ln\frac{1-e_m}{e_m}
$$
（d）更新训练数据集的权值分布
$$
D_{m + 1} = (w_{m+1,1},\cdots, w_{m+1,i},\cdots,w_{m+1,N})\tag{8.3}\\
$$

$$
w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i))\tag{8.4}\\
规范化因子\quad Z_m = \sum_iw_{mi}exp(-\alpha_my_iG_m(x_i))
$$

​		使得 $D_{m+1}$ 成了概率分布，8.4 可以写成
$$
w_{m+1,i} = \begin{cases}
\frac{w_{mi}}{Z_m}e^{-\alpha_m},\quad G_m(x_i) = y_i\\
\frac{w_{mi}}{Z_m}e^{\alpha_m},\quad G_m(x_i) = y_i
\end{cases}
$$
​	被 $G_m(x)$ 误分类的样本的权值得以扩大，而被正确分类的样本的权值会减小，两相比较，误分类样本的权值被放大 $e^{2\alpha_m} = \frac{e_m}{1-e_m}$ 倍。因此误分类数据在下一轮训练中起更大的作用，不改变训练数据，而不断改变训练数据权值的分布，使得训练数据在基分类器的学习中起不同的作用	

（3）构建基分类器的线性组合
$$
f(x) = \sum_m\alpha_mG_m(x)\tag{8.6}
$$
​	最终分类器
$$
G(x) = sign(f(x)) = sign(\sum_m\alpha_mG_m(x))\tag{8.7}
$$
​	此处线性组合 $f(x)$ 实现 M 个基分类器的加权表决，$\sum_m\alpha_m \neq 1$，$f(x)$ 的符号决定了实例的类，绝对值表示分类的确信度。

# 训练误差分析

AdaBoost 最基本的性质是他能在学习的过程中不断减少训练误差，即在训练数据集上的分类误差率

**定理 8.1** AdaBoost 算法最终分类器的训练误差界为
$$
\frac1N\sum_iI(G(x_i)\neq y_i)\leq\frac1N\sum_iexp(-y_if(x_i))=\prod_m Z_m\tag{8.9}
$$
$G(x),f(x),Z_m$ 分别由 8.7，8.6，8.5 给出

证明：当 $G(x_i) \neq y_i$ 时，$y_if(x_i) < 0，so\ exp(-y_if(x_i))\geq 1,I(G(x_i)\neq y_i)=1$，前半部分可推导出来

后半部分要用到
$$
Z_mw_{m+1,i} = w_{mi}exp(-\alpha_my_iG_m(x_i))
$$
推导如下
$$
\frac1N\sum_i exp(-y_if(x_i))\qquad\qquad\qquad\quad\\
=\frac1N\sum_i exp(-\sum_m\alpha_my_iG_m(x_i))\qquad\\
=\frac1N\sum_i\prod_mexp(-\alpha_my_iG_m(x_i))\qquad\ \ \\
—— 指数函数特性\\
=\sum_iw_{1i}\prod_{m}exp(-\alpha_my_iG_m(x_i))\qquad\ \ \\
—— w_{1i} = \frac1N\\
= Z_1\sum_i w_{2i}\prod_{m=2}exp(-\alpha_my_iG_m(x_i)\quad\ \ \\
= Z_1Z_2\cdots Z_M\qquad\qquad\qquad\qquad\qquad\ \ \\
=\prod_m Z_m\qquad\qquad\qquad\qquad\qquad\qquad\quad
$$
这一定理说明，可以在每一轮选取适当的 $G_m$ 使得 $Z_m$ 最小，从而使得训练误差下降最快

对二分类问题，如下

**定理 8.2** 
$$
\prod_mZ_m =\prod_m[2\sqrt{e_m(1-e_m)}]=\prod_m\sqrt{1-4\gamma_m^2}\leq exp(-2\sum_m\gamma_m^2)\\
\gamma_m = \frac12 - e_m
$$
这表明在此条件下，AdaBoost 的训练误差是以指数速率下降的

此算法不需要直到下界 $\gamma$ ，它具有适应性，即它能适应弱分类器各自的训练误差率。这也是该算法名称的由来

# 解释

## 前向分步算法

加法模型（additive model）
$$
f(x) =\sum_m\beta_mb(x;\gamma_m)\tag{8.13}
$$
$b(x;\gamma_m)$ 为基函数，$\gamma_m$ 为基函数参数，$\beta_m$ 为基函数的系数，式 8.6 显然是一个加法模型

在给定训练数据即损失函数 $L(y,f(x))$ 的条件下，学习加法模型 $f(x)$ 成为经验风险极小化即损失函数极小化问题
$$
min_{\beta_m,\gamma_m}\sum_iL(y_i,\sum_m\beta_mb(x;\gamma_m))\tag{8.14}
$$
前向分步算法（foward stagewise algorithm）求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，就可以简化复杂度，具体每步只需优化如下损失函数
$$
min_{\beta,\gamma}\sum_iL(y_i,\beta b(x_i;\gamma))\tag{8.15}
$$
**前向分步算法**

（1）初始化 $f_0(x) = 0$

（2）对 m=1,2,...,M

​		（a）极小化损失函数
$$
(\beta_m,\gamma_m) = argmin_{\beta,\gamma}\sum_iL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma))\tag{8.16}
$$
​		得到参数 $\beta_m,\gamma_m$

​		（b）更新
$$
f_m(x) = f_{m-1}(x) + \beta_m b(x;\gamma_m)\tag{8.17}
$$
（3）得到加法模型
$$
f(x) = f_M(x) = \sum_m\beta_mb(x;\gamma_m)\tag{8.18}
$$

## 前向分步算法与 AdaBoost

**定理 8.3** AdaBoost 为前向分步加法算法的特征，这时，模型是由基分类器组成的加法模型，损失函数为指数函数

证明前向分步算法的损失函数为指数损失函数
$$
L(y,f(x)) = exp[-yf(x)]
$$
时，其学习的具体操作等价于 AdaBoost 的学习

假设在第 m 轮迭代得到 $\alpha_m,G_m(x),f_m(x)$
$$
f_m(x) = f_{m-1}(x) + \alpha_mG_m(x)
$$
目标是使 $f_m(x)$ 在数据集 T 上的指数损失最小，即
$$
(\alpha_m,G_m(x)) = argmin_{\alpha,G}\sum_iexp[-y_i(f_{m-1}(x_i) + \alpha G(x_i)]\\
=argmin_{\alpha,G}\sum_i \bar w_{mi}exp[-y_i\alpha G(x_i)]\tag{8.21}
$$
$\bar w_{mi}=exp[-y_if_{m-1}(x_i)]$ ，不依赖于 $\alpha,G$，与最小化无关，但依赖于 $f_{m-1}(x)$ ，随着每一轮迭代发生改变

证明使 8.21 达到最小的 $\alpha_m^*,G_m(x)^*$ 就是 AdaBoost 算法得到的 $\alpha_m,G_m(x)$

首先，求 $G_m(x)^*$，对任意 $\alpha > 0$，使 8.21 最小的 G(x) 如下得到
$$
G_m(x)^* = argmin_G\sum_i\bar w_{mi}I(G(x_i) \neq y_i)
$$
分类器 $G_m^*(x)$ 即为 AdaBoost 算法的基分类器 $G_m(x)$，因为它是使第 m 轮加权训练数据分类误差率最小的基分类器

求 $\alpha_m^*$
$$
\sum_i\bar w_{mi} exp[-y_i\alpha G(x_i)]\\
=\sum_{G_m(x_i)=y_i}\bar w_{mi}e^{-\alpha} + \sum_{G_m(x_i)\neq y_i}\bar w_{mi}e^\alpha\\
=(e^\alpha - e^{-\alpha})\sum_i\bar w_{mi}I(G_m(x_i)\neq y_i) + e^{-\alpha}\sum_i \bar w_{mi}\tag{8.22}
$$
将 $G_m^*(x)$ 代入，对 $\alpha$ 求导，可得，使 8.21 最小的 $\alpha$
$$
\alpha_m^* = \frac12ln\frac{1-e_m}{e_m}
$$
这里的 $\alpha_m^*$ 与 AdaBoost 中的 $\alpha_m$ 一致

样本权值的更新，由
$$
f_m(x) = f_{m-1}(x) + \alpha_mG_m(x)\\
\bar w_{mi} = exp[-y_if_{m-1}(x_i)]
$$
可得
$$
\bar w_{m+1,i} = \bar w_{m,i} exp[-y_i\alpha_mG_m(x)]
$$
与 AdaBoost 算法样本权值的更新，只相差规范化因子，因此等价

# 提升树

提升树是以分类树或回归树为基本分类器的提升方法，提升树被认为是统计学习中性能最好的方法之一

## 提升树模型

提升方法采用加法模型（基函数的线性组合）与前向分步算法，以决策树为基函数的提升方法成为提升树（boosting tree）。对分类问题决策树是二叉分类树，对回归问题是二叉回归树。上述 Adaboost 中的基分类器 x<v 或 x>v，可以看作是由一个根结点直接连接两个叶结点的简单决策树，即决策树桩（decision stump）。提升树模型可以表示为决策树的加法模型
$$
f_M(x) = \sum_mT(x;\theta_m)\tag{8.23}
$$
$T(x;\theta_m)$ 为决策树，$\theta_m$ 为决策树参数；M 为树的个数

## 提升树算法

首先确定初始提升树 $f_0(x)=0$，第 m 步的模型是
$$
f_m(x) = f_{m-1}(x) + T(x;\theta_m)\tag{8.24}
$$
$f_{m-1}(x)$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $\theta_m$
$$
\hat\theta_m = argmin_{\theta_m}\sum_iL(y_i,f_{m-1}(x_i) + T(x_i;\theta_m))\tag{8.25}
$$
由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂也是如此，所以提升树是一个高功能的学习算法

下面讨论针对不同问题的提升树学习算法，主要区别在于使用的损失函数不同。包括用平方误差损失函数的回归问题，用指数损失函数的分类问题，以及用一般损失函数的一般决策问题

对二分类问题，提升树算法只需将 AdaBoost 算法中的基分类器限制为二分类树即可，下面叙述回归问题的提升树

已知数据集 T，如果将输入空间划分为 $J$ 个互不相交的区域 $R_1,\cdots,R_J$，并在每个区域上确定输出的常量 $c_j$，那么树可表示为
$$
T(x;\theta) = \sum_j c_jI(x\in R_j)\tag{8.26}
$$
参数 $\theta=\{(R_1,c_1),\cdots,(R_N,c_J)\}$ 表示树的区域划分和各区域上的常数，J 为回归树的复杂度即叶结点的个数

前向分步算法：
$$
f_0(x) = 0\\
f_m(x) = f_{m-1}(x) + T(x;\theta_m)\\
f_M(x) = \sum_mT(x;\theta_m)
$$
在第 m 步，给定当前模型 $f_{m-1}(x)$，需求解
$$
\hat\theta_m = argmin_{\theta_m}L(y_i,f_{m-1}(x_i) + T(x_i;\theta_m))
$$
当采用平方误差损失函数时
$$
L(y,f(x)) = (y-f(x))^2\\
L(y,f_{m-1}(x) + T(x;\theta_m))\\
=[y- f_{m-1}(x) - T(x;\theta_m)]^2\\
=(r - T(x;\theta_m))^2\\
r = y - f_{m-1}(x)\tag{8.27}
$$
$r$ 是当前模型拟合数据的残差（residual）。所以，对回归问题的提升树算法，只需简单地拟合当前模型的残差。

**算法 8.3**

（1）初始化 $f_0(x)=0$

（2）对 m = 1，2，...，M

​	（a）计算残差
$$
r_{mi} = y_i - f_{m-1}(x_i)
$$
​	（b）拟合残差学习一个回归树，得到 $T(x;\theta_m)$

​	（c）更新 $f_m(x) = f_{m-1}(x) + T(x;\theta_m)$

（3）得到回归问题提升树
$$
f_M(x) = \sum_{m}T(x;\theta_m)
$$

## 梯度提升

提升树利用加法模型与前向分步算法实现学习的优化过程，当损失函数是平方损失和指数损失函数时，每一步优化是很简单，但对一般损失函数而言，往往并不那么容易，因此 Freidman 提出了梯度提升（gradient boosting）算法，这是最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值
$$
-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x) = f_{m-1}(x)}
$$
作为回归问题提升树算法的残差的近似值，拟合一个回归树

**算法 8.4**

（1）初始化
$$
f_0(x) = argmin_c\sum_iL(y_i,c)
$$
（2）对 m=1,2,...,M

​	（a）对 i=1,2,...,N，计算
$$
r_{mi} = -[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x) = f_{m-1}(x)}
$$
​	（b）对 $r_{mi}$ 拟合一个回归树，得到第 m 棵树的叶结点区域 $R_{ny},j=1,2,...,J$，

​	（c）对 $j = 1,2,...,J$，计算
$$
c_{mj} = argmin_c\sum_{x_i\in Rmj}L(y_i,f_{m-1}(x_i) + c)
$$
​	（d）更新 
$$
f_m(x) = f_{m-1}(x) + \sum_jc_{mj}I(x\in R_{mj})
$$
（3）得到回归树
$$
\hat f(x) = f_M(x) = \sum_m\sum_jc_{mj}I(x\in R_{mj})
$$
