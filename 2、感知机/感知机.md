感知机（perceptron）是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别。感知机对应于输入空间中将实例划分为正负两类的分类超平面，属于判别模型。感知机学习旨在求出将训练数据进行线性划分的分类超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。

# 感知机模型

定义：假设输入空间是 X，输出空间是 Y={+1,-1}。输入 $x\in X$ 表示实例的特征向量，由输入空间到输出空间的如下函数
$$
f(x) = sign(w\cdot x + b)
$$
称为感知机，sign 为符号函数，w 叫做权值或权值向量，b 叫做偏置，$w\cdot x$ 表示 w 和 x 的内积。

感知机模型的假设空间是定义在特征空间上的所有线性分类模型（linear classification model）或线性分类器（linear classifier），即函数集合 $\{f|f(x) = w\cdot x + b\}$

感知机由如下解释：线性方程 $w\cdot x + b = 0$ 对应于特征空间的一个超平面 S，w 为超平面的法向量，b 为超平面的截距。这个超平面将特征空间分为两部分，位于两部分的点或特征向量被分为正负两类，因此，超平面 S 被称为分离超平面（separating hyperplane）。

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210201104851211.png" alt="image-20210201104851211" style="zoom:67%;" />

感知机学习，由训练数据集 T，求得感知机模型，即求得参数 w 和 b。

# 学习策略

## 数据集的线性可分性

对于数据集 T，如果存在分类超平面 S 能够将数据集的正实例点和负实例点完全正确的划分到超平面的两侧，即对于所有 $y_i = +1$ 的实例，都有 $w\cdot x + b >= 0$，对于所有 $y_i = -1$ 的实例，都有 $w\cdot x + b <0$。则称数据集 T 为线性可分数据集（linearly separable data set）

## 学习策略

假设数据集是线性可分的，感知机的学习目标是求得一个使得正实例点和负实例点能够完全被正确分离的分离超平面。为了确定这个分离超平面即确定参数 w 和 b，需定义一个学习策略，即定义损失函数并将损失函数极小化

损失函数的一个自然选择是误分类点的总数，但这样的损失函数不是关于 w 和 b 的连续可导函数，不易优化；另一个选择是误分类点到分离超平面的距离。首先写出输入空间任一点 $x_0$ 到超平面的距离
$$
\frac1{||w||}|w\cdot x + b|
$$
对于误分类点有
$$
-y_i(w\cdot x_i + b) > 0
$$
因此，$x_o$ 到 S 的距离为
$$
-\frac1{||w||}y_i(w\cdot x + b)
$$
假设误分类的集合为 M，则所有误分类点到分离超平面的距离和为
$$
-\frac1{||w||}\sum_{x_i\in M}y_i(w\cdot x + b)
$$
不考虑 $\frac1{||w||}$，则为
$$
L(w,b) = -\sum_{x_i\in M}y_i(w\cdot x + b)
$$
上式即为感知机的损失函数。

显然，上式是非负的，误分类点越少，且误分类点离超平面的距离越小，损失函数值越小。如果没有误分类点，则为 0

感知机的学习策略便是使损失函数 $L(w,b)$ 值最小的参数 w 和 b，即感知机模型

# 感知机学习算法

感知机学习问题转化为损失函数的最优化问题，最优化方法是梯度下降算法，本节叙述感知机学习的具体算法，包括原始形式和对偶形式，并证明数据集线性可分的条件下感知机学习算法的收敛性

## 原始形式

### 算法

输入：给定一个训练数据集 T，学习率 $\eta（0<\eta<=1)$ 。求参数 w 和 b，使其成为以下损失函数极小化问题的解
$$
min_{w,b}L(w,b) = -\sum_{x_i\in M}y_i(w\cdot x + b)
$$
输出：w，b；感知机模型 $f = sign(w\cdot x + b)$

感知机学习算法是误分类点驱动的，具体采用随机梯度下降（SGD）。

1. 随机选择 $w_0$ 和 $b_0$ ，利用梯度下降法不断极小化目标函数。
2. 极小化过程中是一次随机选择一个误分类点使其梯度下降，假设误分类点集合 M 是固定的，那么损失函数的梯度由下列两式给出

$$
\nabla_wL(w,b) = -\sum_{x_i\in M}y_ix_i\\
\nabla_bL(w,b) = -\sum_{x_i\in M}y_i
$$

​			随机选择误分类点 $(x_i,y_i)$，对w，b进行更新
$$
w \leftarrow w + \eta y_ix_i\\
b \leftarrow b + \eta y_i
$$

3. 转至2，直至训练集中没有误分类点。

这种算法直观上解释如下，当一个实例点被误分类时，则调整 w，b的值，使分离超平面向误分类点移动，以减少误分类点到超平面的距离，直至超平面越过误分类点使其能被正确分离。

### 例子

如下图所示数据集，正实例点为 $x_1=(3,3)^T,x_2=(4,3)^T$，负实例点为 $x_3=(1,1)^T$。这里 $w = (w^{(1)},w^{(2)})^T,x = (x^{(1)},x^{(2)})^T$

<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210201124353449.png" alt="image-20210201124353449" style="zoom:67%;" />

解：构建最优化问题
$$
min_{w,b}L(w,b) = -\sum_{x_i\in M}y_i(w\cdot x + b)
$$
按照上述算法求解 w 和 b，$\eta = 1$

1. 取初值 $w_0 = 0,b_0 = 0$

2. 取 $x_1 = (3,3)^T$，有 $y_1(w_0\cdot x_1 + b_0) = 0$，则分类错误，更新 w,b
   $$
   w_1 = w_0 + y_1x_1=(3,3)^T,b_1 = b_0 + y_1 = 1
   $$
   得线性模型
   $$
   w_1\cdot x + b_1 = 3x^{(1)} + 2x^{(2)} + 1
   $$

3. 对 x1,x2，显然 $y_i(w_1\cdot x_i + b_1) > 0$，不更改 w,b；

   对 $x_3 = (1,1)^T$，$y_3(w_1\cdot x_3 + b_1) <0$，被误分类，更新 w,b
   $$
   w_2 = w_1 + y_3x_3 = (3,3)^T - (1,1)^T = (2,2)^T\\
   b_2 = b_1 + y_3 = 0\\
   得到线性模型\qquad w_2\cdot x + b_2 = 2x^{(1)} + 2x^{(2)}
   $$
   如此继续下去，直到

$$
w_7 = (1,1)^T,b_7 = -3\\
w_7\cdot x + b_7 = x^{(1)} + x^{(2)} - 3
$$

​			对所有数据点都有 $y_i(w_7\cdot x_i + b_7) > 0$，没有误分类点，损失函数值达到最小。

​			感知机模型为 $ f(x) = sign(x^{(1)} + x^{(2)} - 3)$。

​			迭代过程见下表<img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210201130907583.png" alt="image-20210201130907583" style="zoom:67%;" />

感知机若选取不同的初值或选取不同的误分类点，解可以不同。

### 代码如下

[感知机](D:\PythonProject\AI项目\二、感知机\感知机原始形式.py)

## 算法的收敛性

为了叙述和推导，将偏置 b 并入权重向量 w，记作 $\hat w = (w^T,b)^T$，则 $\hat x = (x^T,1)^T$，显然 $\hat w\cdot \hat x = w\cdot x + b$

### 定理

假设数据集 T 是线性可分的

1. 存在满足条件 $||\hat w_{opt}|| = 1$ 的超平面 $\hat w_{opt} \cdot \hat x = w_{opt}\cdot x + b_{opt}= 0$ 使得数据集完全分开。且存在 $\gamma >0$，使得
   $$
   y_i(\hat w_{opt} \cdot \hat x) = y_i(w_{opt}\cdot x + b_{opt}) \geq \gamma
   $$

2. 令 $R = max||x_i||$，则感知机在训练集的误分类次数 k 满足不等式
   $$
   k \leq (\frac R{\gamma})^2
   $$

证明：

（1）对于线性可分数据集，取超平面为 $\hat {w_{opt}}\cdot\hat x=0$，使 $||\hat w_{opt}||=1$，均有 $y_i(\hat w_{opt}\cdot\hat x)>0$，所以存在 $\gamma = min(y_i(\hat w_{opt} \cdot \hat x))$，即 $y_i(\hat w_{opt} \cdot \hat x) = y_i(w_{opt}\cdot x + b_{opt}) \geq \gamma$

（2）感知机算法从 $\hat w_0 = 0$ 开始，如果实例被误分类，则更新权重，则 $\hat w_{k-1}$ 为第 k 个被误分类的实例之前的扩充权重向量
$$
\hat w_{k - 1} = (w_{k-1}^T,b_{k-1})^T
$$
​			对于误分类的实例，有 $y_i(w_{k -1}\cdot x + b_{k-1})\leq0$，更新权重
$$
w_k = w_{k-1} + \eta y_i x_i\\
b_k = b_{k-1} + \eta y_i\\
则\qquad \hat w_k = \hat w_{k-1} + \eta y_i \hat x_i
$$
​		下面证明两个不等式
$$
（1）\hat w_k\cdot\hat w_{opt} \geq k\eta\gamma\\
由上式可得\\
\hat w_k\cdot\hat w_{opt} = \hat w_{k - 1}\cdot\hat w_{opt} + \eta y_i \hat x_i\cdot w_{opt}\\
\geq  \hat w_{k - 1}\cdot\hat w_{opt} + \eta\gamma\\
\ \geq \hat w_{k - 2}\cdot\hat w_{opt} + 2\eta\gamma\\
\geq k\eta\gamma\qquad\qquad\quad
$$

$$
（2）||\hat w_k||^2 \leq k\eta^2\gamma^2\\
由前面的式子可得\\
||\hat w_k||^2 = ||w_{k-1}||^2 + 2\hat w_{k-1}\eta y_i\hat x_i + \eta^2||\hat x_i||^2\\
\leq||w_{k-1}||^2 + \eta^2||\hat x_i||^2\qquad\quad\\
\leq||w_{k-1}||^2 + \eta^2R^2\quad\qquad\quad\\
\leq k\eta^2R^2\qquad\qquad\qquad\qquad\ 
$$

​	联立上面两个不等式可得
$$
k\eta\gamma\leq\hat w_k\cdot\hat w_{opt}\leq||\hat w_k||\ ||\hat w_{opt}||\leq\sqrt k\eta R\\
则有 k\leq(\frac R\gamma)^2
$$
定理表明，误分类次数 k 是有上界的，经过有限次迭代后可以找到将训练数据完全分开的超平面，也就是说如果数据线性可分，感知机学习算法的原始形式是收敛的，如果数据集线性不可分，算法不收敛，迭代结果会震荡。

## 对偶形式

对偶形式的想法是将 w 和 b 表示为实例 $x_i$ 和 标签 $y_i$ 的线性组合的形式，通过求解系数而求得 w 和 b。不失一般性，假设初值 $w_0=b_0 = 0$，对误分类点通过
$$
w = w + \eta y_ix_i\\
b = b + \eta y_i
$$
逐步修改，设修改 n 次，则 w，b 的增量分布为 $\alpha_iy_ix_i$ 和 $\alpha_iy_i$，此处 $\alpha_i = n_i\eta$。则 w，b 可表示为
$$
w = \sum_{i=1}^N \alpha_iy_ix_i\\
b = \sum_{i=1}^N \alpha_iy_i
$$
这里 $\alpha\geq 0，i=1,2,...,N$，当 $\eta = 1$ 时，表示第 i 个实例点由于误分而进行更新的次数，实例点更新次数越多，说明它离超平面越近，越难正确区分，这样的实例对学习影响很大

### 算法

输入：线性可分数据集 T

输出：$\alpha，b$，感知机模型 $f(x) = sign(\sum_{j=1}^N\alpha_jy_jx_j\cdot x + b)$，其中 $\alpha = (\alpha_1,\alpha_2,...,\alpha_N)^T$

1. $\alpha = b = 0$

2. 在训练集中选取 $(x_i,y_i)$

3. 如果 $y_i(\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i + b) \leq 0$，则分类错误(更新 $\alpha$ 时，是更新 $x_i$ 对应的 $\alpha_i$，其余不更新)
   $$
   \alpha \leftarrow \alpha + \eta\\
   \# b \leftarrow b + \eta y_i
   $$

4. 直到 2 没有误分类数据

对偶形式中训练数据仅以内积形式出现，为了方便，可将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵为 Gram 矩阵，
$$
Gram = [x_i\cdot x_j]_{N*N}
$$
在计算 $\sum_{j=1}^N\alpha_jy_jx_j\cdot x_i = \alpha_1y_1x_1\cdot x_i + \alpha_2y_2x_2\cdot x_i + \alpha_3y_3x_3\cdot x_i $，其中 i 从 1 ~ N，$\alpha_iy_i$ 为常数，后面的 $(x_1-x_N)\cdot x_i$ 会跟每个样本内积，所以会计算
$$
\alpha_1y_1x_1\cdot x_1 + \alpha_2y_2x_2\cdot x_1 + \alpha_3y_3x_3\cdot x_1\\
\alpha_1y_1x_1\cdot x_2 + \alpha_2y_2x_2\cdot x_2 + \alpha_3y_3x_3\cdot x_2\\
\vdots\\
\alpha_1y_1x_1\cdot x_N + \alpha_2y_2x_2\cdot x_N + \alpha_3y_3x_3\cdot x_N
$$
即
$$
[\alpha_1y_1,\alpha_2,y_2,...,\alpha_Ny_N]\cdot \left[
\begin{matrix}
x_1x_1 & x_1x_2 & \dots & x_1x_N\\
x_2x_1 & x_2x_2 & \dots & x_2x_N \\
\dots & \dots &\dots & \dots\\
x_Nx_1 & x_Nx_2 & \dots & x_Nx_N
\end{matrix}
\right]
$$

### 例

数据同上面的例子，此处利用对偶形式求解

1. 取 $\alpha_i = 0,i=1,2,3,b=0,\eta=1$

2. 计算Gram矩阵
   $$
   Gram =\left[\begin{matrix}
   18 & 21 & 6\\
   21 & 25 & 7\\
   6 & 7 & 2
   \end{matrix}\right]
   $$

3. 误分条件
   $$
   y_i(\sum_{j=1}^N\alpha_jy_jx_j\cdot x + b) \leq 0
   $$
   参数更新
   $$
   \alpha_i \leftarrow \alpha_i + \eta\\
   b \leftarrow b + \eta y_i
   $$
   $x_1 = (3,3)^T$，显然判别条件为 0，误分

   更新
   $$
   \alpha_1 = 1,\alpha_2 = 0,\alpha_3 = 0\\
   b = 1
   $$
   实例为 x2 时，判别条件大于 0，不更新

   $x_3 = (1,1)^T$，判别条件小于 0，误分

   更新
   $$
   \alpha_1 = 1,\alpha_2 = 0,\alpha_3 = 1\\
   b = 0
   $$
   迭代，过程略

4. 最后得到结果为
   $$
   w =\sum_{i=1}^N\alpha_iy_ix_i = 2x_1 + 0x_2 - 5x_3 = (1,1)^T\\
   b = -3
   $$
   得到分离超平面及模型与前面相同

   求解过程如下

   <img src="C:\Users\you\AppData\Roaming\Typora\typora-user-images\image-20210202192038744.png" alt="image-20210202192038744" style="zoom:67%;" />

代码如下：[对偶形式](D:\PythonProject\AI项目\二、感知机\感知机对偶形式.py)

# 习题
2.1 感知机不能表示异或

异或表示如下

|  x1  |  x2  | x1$\bigoplus$x2 |
| :--: | :--: | :-------------: |
|  0   |  0   |        0        |
|  0   |  1   |        1        |
|  1   |  0   |        1        |
|  1   |  1   |        0        |

$$
y = sign(wx + b) = sign(z)= \begin{cases}
1, z >= 0\\
-1,z < 0
\end{cases}
$$

分别带入 x 有两个维度

（0，0），要想 y = 0，有 b < 0

（0，1），要想 y = 1，有  w2 + b > 0，w2 > -b > 0

（1，0），要想 y = 1，有 w1 + b > 0，w1> -b > 0

（1，1），要想 y = 0，有 w1 + w2 + b < 0

根据前三个条件 w1 + w2 + b > 0，与第四个条件冲突，所以，感知机不可表示异或





*参考 https://www.jianshu.com/p/e79169493d75，统计学习方法（三）感知机为什么不能表示异或，[shijiatongxue](https://www.jianshu.com/u/8b5b6ac12174)*







