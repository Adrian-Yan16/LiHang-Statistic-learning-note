朴素贝叶斯（naive Bayes）是基于贝叶斯定理与特征条件独立假设的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布，然后基于此模型，对于给定的输入 x，利用贝叶斯定理求出后验概率最大的输出 y。

# 学习与分类

## 基本方法

X 为输入空间中的随机向量，Y 为输出空间中的随机向量，P(X|Y) 是 X 和 Y 的联合概率分布，给定数据集 T 由 X，Y独立同分布产生

朴素贝叶斯通过训练数据集学习联合概率分布，具体地，学习以下先验概率分布和条件概率分布，先验概率分布
$$
P(Y=c_k)
$$
条件概率分布
$$
P(X=x|Y=c_k) = P(X^1=x^1,X^2=x^2,\cdots|Y=c_k)\\
根据条件独立性假设得\qquad =\prod(X^j=x^j|Y=c_k)
$$
假设 $x^{(j)}$ 可取值有 $S_j$ 个，j=1,2,...,n，Y 可取值有 K 个，则参数数量为 $K\prod S_j$

朴素贝叶斯实际上学习到的是生成数据的机制，属于生成模型，条件独立性假设等于是说用于分类的特征在类确定的情况下是条件独立的，这一假设使朴素贝叶斯法变得简单，但有时会牺牲一些准确性

分类时，对于给定的输入 x，求出后验概率分布$P(Y=c_k|X=x)$，将后验概率最大的类作为 x 的类输出。计算根据贝叶斯定理进行
$$
P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum P(X=x|Y=c_k)P(Y=c_k)}\\
=\frac{\prod P(X^j=x^j|Y=c_k)P(Y=c_k)}{\sum P(Y=c_k)\prod P(X^j=x^j|Y=c_k)}
$$
分类器为
$$
y=f(x)=argmax_{c_k}\frac{\prod P(X^j=x^j|Y=c_k)P(Y=c_k)}{\sum \prod P(X^j=x^j|Y=c_k)P(Y=c_k)}\\
因为对于每个c_k,分母都是一样的所以\quad\\
=argmax_{c_k}P(Y=c_k)\prod P(X^j=x^j|Y=c_k)
$$

## 后验概率最大化的含义

朴素贝叶斯将实例分到后验概率最大的类别，实际上相当于期望风险最小化，假设选择0-1损失函数
$$
L(y,f(x)) = \begin{cases}
1,\qquad y\neq f(x)\\
0,\qquad y=f(x)
\end{cases}
$$
f(x) 为分类决策函数

期望风险为
$$
R_{exp}(f) = E[L(y,f(X))]\\
期望是对 P(X|Y) 取得，所以\\
=E_X[L(c_k,f(X))P(c_k|X)]
$$
为了使期望风险最小化，只需要逐个将 X=x 极小化级可
$$
f(x) = argmin\sum L(c_k,y)P(c_k|X=x)\\
=argmin\sum P(y\neq c_k|X=x)\\
=argmin(1-P(y=c_k|X=x))\\
=argmax\ P(y=c_k|X=x)
$$
这样一来，就由期望风险最小化得到了后验概率最大化准则，即朴素贝叶斯采用的原理

# 参数估计

## 极大似然估计

学习意味着估计 $P(Y=c_k)$ 和 $P(X^j=x^j|Y=c_k)$，可以应用极大似然估计法估计相应的概率
$$
P(Y=c_k) = \frac{\sum I(y=c_k)}{N}
$$
设第 j 个特征 $x^j$ 可能取值的集合为 $a_{jl} = \{a_{j1},a_{j2},\cdots,a_{jS_j}\}$，条件概率的极大似然估计为
$$
P(X^j=a_{jl}|Y=c_k) = \frac{\sum I(x_i^j=a_{jl},y_i=c_k)}{\sum I(y_i=c_k)}\\
j=1,2,\cdots,n\quad l = 1,2,\cdots,S_j\quad k=1,2,\cdots,K
$$
式中，$x_i^j$ 是第 i 个样本的第 j 个特征，$a_{jl}$ 是第 j 个特征可能取得第 l 个值，$I$ 为指示函数

### 推导

#### 先验概率

设$P(Y=c_k)=p，m=\sum I(y_i=c_k)$ 
$$
似然函数为 L_p =\left( \begin{matrix}
N\\
m
\end{matrix}\right)p^m(1-p)^{(N-m)}\\
等式两边分别对p求微分得\\
mp^{(m-1)}(1-p)^{(N-m)} - (N-m)p^m(1-p)^{N-m-1}=0\\
p^{m-1}(1-p)^{N-m-1}(m-Np) = 0\\
可得p=0,p=1,p=\frac{m}{N}
$$

#### 条件概率

设$P(X^j=x^j|Y=c_k)=p,\sum I(x_i^j=a_{jl},y_i=c_k)=q$
$$
似然函数为 L_p =\left( \begin{matrix}
m\\
q
\end{matrix}\right)p^q(1-p)^{(m-q)}\\
等式两边分别对p求微分得\\
qp^{q-1}(1-p)^{m-q} -(m-q)p^q(1-p)^{m-q-1} = 0\\
p^{q-1}(1-p)^{m-q-1}(q-mp)=0\\
得p=0,p=1,p=\frac{q}{m}
$$

## 贝叶斯估计

用极大似然估计可能会出现所要估计得概率为 0 的情况，这时会影响到后验概率的计算，影响分类的结果。解决这一问题的方法是采用拉普拉斯平滑（Laplace Smoothing），条件概率的贝叶斯估计为
$$
P_{\lambda}(X^j=a_{jl}|Y=c_k)=\frac{\sum I(x_i^j=a_{jl},y_i=c_k) + \lambda}{\sum I(y_i=c_k) + S_j\lambda}
$$
常取 $\lambda = 1$，先验概率的贝叶斯估计为
$$
P_{\lambda}(Y=c_k) = \frac{\sum I(y_i=c_k)+\lambda}{N+K\lambda}
$$

### 推导

#### 先验概率

在没有任何信息的情况下，可以假设先验概率为均匀概率

即 $p=\frac{1}{K} \rightarrow pK-1=0$ 由上面的公式可知 $pN-\sum I(y_i=c_k) = 0$

存在参数 $\lambda$ 使得
$$
(pK-1)\lambda +pN-\sum I(y_i=c_k) = 0\\
可得\qquad P(Y_i=c_k)=p=\frac{\sum I(y_i=c_k) +\lambda}{N+K\lambda}
$$

#### 条件概率

同上可得$P(X^j=a_{jl},Y_i=c_k)=\frac{\sum I(x_i^j=a_{jl},y_i=c_k) + \lambda}{N+KS_j\lambda}$
$$
P(X^j=a_{jl}|Y_i=c_k) = \frac{\frac{\sum I(x_i^j=a_{jl},y_i=c_k) + \lambda}{N+KS_j\lambda}}{\frac{\sum I(y_i=c_k) +\lambda}{N+K\lambda}}\\
\lambda 可取任意值，令\lambda=S_j\lambda\\
=\frac{\frac{\sum I(x_i^j=a_{jl},y_i=c_k) + \lambda}{N+KS_j\lambda}}{\frac{\sum I(y_i=c_k) +S_j\lambda}{N+KS_j\lambda}}\\
=\frac{\sum I(x_i^j=a_{jl},y_i=c_k) + \lambda}{\sum I(y_i=c_k) +\lambda}\\
=\frac{\sum I(x_i^j=a_{jl},y_i=c_k) + \lambda}{\sum I(y_i=c_k) +S_j\lambda}
$$

# 高斯朴素贝叶斯

特征的可能性被假设为高斯-GaussianNB

适合用于样本的值是连续的，数据呈正态分布的情况（比如人的身高、城市家庭收入、一次考试的成绩等等）

数学期望（mean）：$\mu$

方差：$\sigma^2:\frac{\sum(X-\mu)^2}{N}$

概率密度函数：
$$
P(x_i|c_k) = \frac1{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$


MultinominalNB（多项式朴素贝叶斯分类器）：适合用于大部分属性为离散值的数据集

BernoulliNB（伯努利朴素贝叶斯分类器）：适合用于特征值为二元离散值或是稀疏的多元离散值的数据集











